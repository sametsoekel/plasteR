outliers<-k$out
z<-which(x%in% outliers)
return(z)
}
else{
return(NA)
}
}
univariateoutliers<-lapply(df, univariatedetect)
######### keeping univariate outliers #########
########## making clean data ##########
if(return_clean_data==T){
clean_data <- dfn_nona[!mv_outliers,]
}
else{
clean_data <- data.frame()
}
########## making clean data ##########
return(
list(
"univariate_outliers"=univariateoutliers,
"mv_outliers"=clean_data,
"normality_test"=nvector
)
)
}
outlier.outline <- function(data,plot_show = TRUE,type="den",get_skew=TRUE,get_normality=TRUE,
normality_as="log",return_clean_data=TRUE){
####### plot par reseters func. #######
opar <- par()
####### plot par reseters func. #######
####### plot margin arbiter func.-start ########
margin_arbiter <-function(x){
if(sqrt(x) == round(sqrt(x))){
xlim <- sqrt(x)
ylim <- sqrt(x)
return(list("xlim"=xlim,"ylim"=ylim))
}
else{
xlim <- sqrt(x)
xlim <- floor(xlim)
ylim <- xlim + 1
if(xlim*ylim<x){
return(list("xlim"=xlim+1,"ylim"=ylim))
}
else{
return(list("xlim"=xlim,"ylim"=ylim))
}
}
}
####### margin arbiter func.-end ########
#########################################################################################################
####### some statistical calculations via MVN and mvoutlier ######
#### a part of report output ####
cat("plasteR Outlier Outline\n\n\n")
#### a part of report output ####
options(warn=-1) ## warn hider
df <- data.frame(data)
dfn <- purrr::keep(df,is.numeric)
dfn_nona <- na.omit(dfn)
results_mvo<-mvoutlier::aq.plot(purrr::keep(dfn_nona,is.numeric))
cat("\n\n\n")
if(nrow(dfn)>=5000 || nrow(dfn)<3) {
stop("Sample size should be between 3 and 5000")}
results <- MVN::mvn(dfn)
var_names <- variable.names(dfn,showOutliers=TRUE)
skewness <- results$Descriptives$Skew
normality_test <- results$univariateNormality$Normality
normality_p_value <- as.numeric(results$univariateNormality$`p value`)
mv_outliers <- results_mvo[["outliers"]]
####### some statistical calculations via MVN and mvoutlier #######
#########################################################################################################
######## finding optimal plot margins #########
margin <- margin_arbiter(length(dfn))
xlim <- margin$xlim
ylim <- margin$ylim
######## finding optimal plot margins #########
#########################################################################################################
######## below plots  ########
if(type=="den"){
if(plot_show==TRUE){
plotter_den <- function(x,var){
den <- density(x[[var]],na.rm = T)
####### finding the p value and skewness indexes ########
skw <- skewness[which(var_names==var)]
ntest <- normality_test[which(var_names==var)]
#####
lenx <- max(x[[var]],na.rm = T)-min(x[[var]],na.rm = T)
lim1 <- min(x[[var]],na.rm = T)-lenx
lim2 <- max(x[[var]],na.rm = T)+lenx
####### finding the p value and skewness indexes ########
################# checking given logical arguments ######################
if(get_skew==T){
skw_txt <- c("Skewness Coef.:",toString(skw))
}
else{
skw_txt <- c("","")
}
if(get_normality==T){
nrm_txt <- c("Normality Test :",toString(ntest))
}
else{
nrm_txt <- c("","")
}
text_vector <- c(skw_txt,nrm_txt)
color_vector <- c("dodgerblue","dodgerblue",
"darkblue","darkblue")
################# checking given logical arguments ######################
if (!anyNA(x[[var]])){
plot(den,
main = c(var),
xlab = "",
ylab = "",
xlim=c(lim1,lim2),
col.main=c("black"))
legend("topright",text_vector,
bty = "n",cex = .9,text.col = color_vector)
}
else{
plot(den,
main = c(var," (including NA's)"),
xlab = "",
ylab = "",
xlim=c(lim1,lim2),
col.main="red")
legend("topright",text_vector,
bty = "n",cex = .9,text.col = color_vector)
}
}
par(mfrow=c(xlim,ylim),mar=c(2.3,2.3,2.3,1))
for(vn in var_names) plotter_den(dfn, vn)
par(opar)
}
}
else if(type=="box"){
if(plot_show==TRUE){
par(opar)
boxplot(df,horizontal = T)
}
}
####### normality result vector ########
normality_vector<-MVN::mvn(purrr::keep(df,is.numeric))
if(normality_as=="log"){
nvector <- normality_vector$univariateNormality$Normality
}
else if(normality_as=="pval"){
nvector <- normality_vector$univariateNormality$`p value`
}
else if(normality_as=="NULL"){
nvector <- NULL
}
else{
stop("argument 'normality_as' should be 'log','pval' or 'NULL'")
}
names(nvector) <- normality_vector$univariateNormality$Variable
####### normality result vector ########
######## top plots ########
######### keeping univariate outliers #########
univariatedetect <- function(x){
if(is.numeric(x)){
k<-boxplot.stats(x)
outliers<-k$out
z<-which(x%in% outliers)
return(z)
}
else{
return(NA)
}
}
univariateoutliers<-lapply(df, univariatedetect)
######### keeping univariate outliers #########
########## making clean data ##########
if(return_clean_data==T){
clean_data <- dfn_nona[!mv_outliers,]
}
else{
clean_data <- data.frame()
}
########## making clean data ##########
return(
list(
"univariate_outliers"=univariateoutliers,
"mv_outliers"=clean_data,
"normality_test"=nvector
)
)
}
install.packages("xgboost")
install.packages("class")
iris
View(iris)
View(iris3[1:25,,1])
View(iris3[1:25,,2])
train <- iris
train <- iris[,c(1,2,3,4)]
View(train)
train <- iris[1:25,c(1,2,3,4)]
train_label <- iris[1:25,5]
View(train_label)
View(iris)
train_label <- iris[1:75,5]
View(train_label)
test <- iris[76:100,c(1,2,3,4)]
View(test)
test_label <- iris[76:100,5]
View(test_label)
library(class)
model<-knn(train,test,train_label)
cl <- factor(c(rep("s",25), rep("c",25), rep("v",25)))
View(cl)
train_label <- iris[1:75,5]
model<-knn(train,test,train_label)
train_label <- data.frame(iris[1:75,5])
train_label <- iris[1:75,5]
train <- iris[1:75,c(1,2,3,4)]
model<-knn(train,test,train_label)
model
mtcars
View(mtcars)
df<-mtcars
mtcars[,sample(1:32,20)]
mtcars[sample(1:32,20),]
train<-mtcars[sample(1:32,20),]
View(train)
train_x<-train[,-c("carb")]
train_x<-train[-c("carb")]
train_x<-train[,c(11)]
View(train)
train_x<-train[,-c(11)]
View(train_x)
train_x <- train[,-c(11)]
train_y <- train$carb
train_y <- as.factor(train$carb)
mtcars-train
train %in% mtcars
mtcars %in% mtcars
mtcars %in% train
train<-mtcars[1:25,]
train_x <- train[,-c(11)]
train_y <- as.factor(train$carb)
test_x <- mtcars[26:33,]
View(test_x)
test_x <- mtcars[26:32,]
test_x <- mtcars[26:32,-c(11)]
test_y <- mtcars[26:32,11]
model <- knn(train_x,test_x,train_y)
model
cbind(model,test_y)
model <- knn(train_x,test_x,train_y,k=20)
cbind(model,test_y)
model <- knn(train_x,test_x,train_y,k=5)
cbind(model,test_y)
model <- knn(train_x,test_x,train_y,k=3)
cbind(model,test_y)
model <- knn(train_x,test_x,train_y,k=4)
cbind(model,test_y)
train<-mtcars[1:25,]
train_x <- train[,-c(11)]
train_y <- train$carb
test_x <- mtcars[26:32,-c(11)]
test_y <- mtcars[26:32,11]
model <- knn(train_x,test_x,train_y,k=4)
model
df <- iris
View(df)
df <- iris[,-c("Species")]
df <- iris[-c("Species")]
df <- iris[-c("Species"),]
df <- iris[,-5]
train <- df[1:75,-5]
View(train)
train <- df[1:75,-6]
train <- df[1:75,-4]
train_x <- df[1:100,-4]
train_y <- df[1:100,4]
test_x <- df[101:150,-4]
train_x <- df[1:100,-4]
train_y <- df[1:100,4]
test_x <- df[101:150,-4]
test_y <- df[101:150,4]
knn(train_x,test_x,train_y)
model<-knn(train_x,test_x,train_y)
View(
cbind(
"gercek"=test_y,
"tahmin"=model
)
)
model<-knn(train_x,test_x,train_y)
model
View(
cbind(
"gercek"=test_y,
"tahmin"=model
)
)
View(model)
tahmin<-knn(train_x,test_x,train_y)
View(
cbind(
"gercek"=test_y,
"tahmin"=tahmin
)
)
View(tahmib)
View(tahmin)
View(tahmin,test_y)
View(data.frame(tahmin,test_y))
sonuc <- data.frame(
"gercek"=test_y,
"tahmin"=tahmin
)
View(sonuc)
sonuc$gercek
sonuc$gercek-sonuc$tahmin
as.numeric(sonuc$gercek)
as.numeric(sonuc$gercek)-as.numeric(sonuc$tahmin)
as.numeric(sonuc$gercek)-as.numeric(sonuc$tahmin)
tahmin
as.numeric(sonuc$tahmin)
sonuc$tahmin
as.charachter(sonuc$tahmin)
as.character(sonuc$tahmin)
as.numeric(
as.character(sonuc$tahmin)
)
thmn<-as.numeric(
as.character(sonuc$tahmin)
)
sonuc$gercek
sonuc$gercek-thmn
(sonuc$gercek-thmn)**2
squarederror<-(sonuc$gercek-thmn)**2
mean(squarederror)
mse<-mean(squarederror)
sqrt(mse)
install.packages("caret")
library(xgboost)
xgb.DMatrix(train_x)
xgb.DMatrix(as.matrix(train_x))
xgbtrainx<-xgb.DMatrix(as.matrix(train_x))
xgbtrainy<-xgb.DMatrix(as.matrix(train_y))
xgb.train(data=xgbtrainx,verbose = 1,label=xgbtrainy)
xgb.train(data=xgbtrainx,label=xgbtrainy)
xgb.train(data=xgbtrainx,label=train_y)
xgb.train(data=xgbtrainx,label=train_y,nrounds = 200)
m<-xgb.train(data=xgbtrainx,label=train_y,nrounds = 200)
m<-xgb.train(data=xgbtrainx,label=xgbtrainy,nrounds = 200)
xgbtrainy<-xgb.DMatrix(train_y)
as.matrix(train_y)
View(as.matrix(train_y))
train_y
xgbtrainy<-xgb.DMatrix(as.matrix(train_y))
m<-xgb.train(data=xgbtrainx,label=xgbtrainy,nrounds = 200)
m<-xgb.train(data=xgbtrainx,label=xgbtrainy,nrounds = 50)
m<-xgb.train(data=xgbtrainx,label=train_y,nrounds = 50)
xgbtrainx<-xgb.DMatrix(as.matrix(train_x),label=train_y)
m<-xgb.train(data=xgbtrainx,nrounds = 50)
m
m$params
m$params$validate_parameters
m$feature_names
m$niter
testxgb<-xgb.DMatrix(as.matrix(test_x))
predict(m,testxgb)
library(caret)
predict(m,testxgb)
View(
data.frame(
"pred"=predict(m,testxgb),
"obs"=test_y
)
)
hata<-data.frame(
"pred"=predict(m,testxgb),
"obs"=test_y
)
defaultSummary(hata)
controller<-trainControl(method = "cv",number = 10)
controller$method
controller<-trainControl(method = c("cv","boot"),number = 10)
controller$method
controller$classProbs
controller<-trainControl(method = c("cv","boot"),number = 10,classProbs = T)
controller<-trainControl(method = c("cv","boot"),number = 10,classProbs = T,summaryFunction = twoClassSummary)
controller<-trainControl(method = c("cv","boot"),number = 10,classProbs = T,summaryFunction = twoClassSummary)
iris
train_index <- createDataPartition(iris$Species,
p = .7,
list = F,
times = 1)
train <- iris[train_index,]
test <- iris[-train_index,]
train_y <- train$Species
train
train_x <- train[,c(1,2,3,4)]
test_y <- test$Species
test_x <- test[,c(1,2,3,4)]
View(train)
View(train_x)
controller<-trainControl(method = c("cv","boot"),number = 10,classProbs = T,summaryFunction = twoClassSummary)
paramlist <- expand.grid(eta = c(0.05,0.03,0.02),
nrounds = c(50, 75, 100),
max_depth = 1:7,
min_child_weight = c(2.0,2.25),
colsample_bytree = c(0.3,0.4,0.5),
gamma = 0,
subsample = 1)
paramlist <- expand.grid(eta = c(0.05,0.03,0.02),
nrounds = c(50, 75, 100),
max_depth = 1:7,
min_child_weight = c(2.0,2.25),
colsample_bytree = c(0.3,0.4,0.5),
gamma = 0,
subsample = 1)
View(paramlist)
xgbmodel<-train(train_x,train_y,
method ="xgb",
tuneGrid = paramlist,
preProcess = "scale",
trControl = controller,
metric = "RMSE",
maximize = FALSE
)
xgbmodel<-train(train_x,train_y,
method ="xgbTree",
tuneGrid = paramlist,
preProcess = "scale",
trControl = controller,
metric = "RMSE",
maximize = FALSE
)
xgbmodel<-train(train_x,train_y,
method ="xgbTree",
tuneGrid = paramlist,
preProcess = "scale",
trControl = controller,
metric = "Accuracy",
maximize = FALSE
)
levels(train$Species) <- make.names(
levels(
factor(
train$Species
)
)
)
train
xgbmodel<-train(Species~.,data=train
method ="xgbTree",
tuneGrid = paramlist,
preProcess = "scale",
trControl = controller,
metric = "Accuracy",
maximize = FALSE
)
xgbmodel<-train(Species~.,data=train,
method ="xgbTree",
tuneGrid = paramlist,
preProcess = "scale",
trControl = controller,
metric = "Accuracy",
maximize = FALSE
)
levels(train$Species) <- make.names(
levels(
factor(
train$Species
)
)
)
xgbmodel<-train(Species~.,data=train,
method ="xgbTree",
tuneGrid = paramlist,
preProcess = "scale",
trControl = controller,
metric = "Accuracy",
maximize = FALSE
)
xgbmodel<-train(Species~.,data=train,
method ="xgbTree",
tuneGrid = paramlist,
preProcess = "scale",
trControl = controller,
metric = "ROC",
maximize = FALSE
)
controller<-trainControl(method ="cv",number = 10,classProbs = T,summaryFunction = twoClassSummary)
xgbmodel<-train(Species~.,data=train,
method ="xgbTree",
tuneGrid = paramlist,
preProcess = "scale",
trControl = controller,
metric = "ROC",
maximize = FALSE
)
controller<-trainControl(method ="cv",number = 10,classProbs = T,summaryFunction = defaultSummary)
paramlist <- expand.grid(eta = c(0.05,0.03,0.02),
nrounds = c(50, 75, 100),
max_depth = 1:7,
min_child_weight = c(2.0,2.25),
colsample_bytree = c(0.3,0.4,0.5),
gamma = 0,
subsample = 1)
xgbmodel<-train(Species~.,data=train,
method ="xgbTree",
tuneGrid = paramlist,
preProcess = "scale",
trControl = controller,
metric = "ROC",
maximize = FALSE
)
